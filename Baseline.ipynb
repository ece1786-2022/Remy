{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mrk6IqBwlih9","executionInfo":{"status":"ok","timestamp":1668292428530,"user_tz":300,"elapsed":1492,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"f8797913-ed27-4c57-9252-6bb39227b225"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["# The code below is needed for using Google Colab, so un comment this if that is what you're using\n"," \n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ECE1786 Project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKB-kEQAEHAz","executionInfo":{"status":"ok","timestamp":1668814030047,"user_tz":300,"elapsed":1725,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"d8d9a320-a830-480d-be7a-7f2c7ed47fc5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/ECE1786 Project\n"]}]},{"cell_type":"code","source":["import torch \n","import numpy as np\n","\n","from nltk.tokenize import sent_tokenize \n","\n","from pathlib import Path \n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from mingpt.bpe import BPETokenizer \n","from mingpt.utils import set_seed \n","import pandas as pd\n","set_seed(1234)"],"metadata":{"id":"wPo9HewGmxcw","executionInfo":{"status":"ok","timestamp":1668814037524,"user_tz":300,"elapsed":5114,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class RecipeDataset(Dataset):\n","    def __init__(self,  truncation=-1):\n","        df = pd.read_pickle(\"/content/drive/MyDrive/ECE1786 Project/cocktail_dataset.pkl\")  \n","        recipes = []\n","        for i in range(len(df.index)):\n","            recipe = \"RECIPE NAME\\n\" + df.loc[i, \"Name\"] + \" \\n\\nRECIPE INGREDIENTS\\n\"\n","            for ingredient in df.loc[i, \"Ingredients\"]:\n","                recipe += ingredient + \"\\n\"\n","            recipe += \"\\nRECIPE INSTRUCTIONS\\n\" \n","            for instruction in  df.loc[i, \"Instructions\"]:\n","                recipe += instruction + \"\\n\"\n","            recipes.append(recipe)\n","\n","        # Tokenize\n","        self.tokenizer = BPETokenizer()\n","        self.data = []  # List of 1-d pytorch tensor\n","        for sent in recipes:\n","            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n","            if truncation >= 0:\n","                self.data.append(tokenized[:truncation])\n","            else:\n","                self.data.append(tokenized)\n","\n","        # Count some items\n","        self.max_sentence_length = np.max([len(d) for d in self.data])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def get_vocab_size(self):\n","        \"\"\"\n","        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), \n","        but actually, only a small number of vocab is used, especially for the small text. \n","        \"\"\"\n","        return 50257\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        The output should be a tuple x and y, both as pytorch tensors.\n","        Please refer to the `run()` method in the mingpt/trainer.py script for \n","        how the x and y are going to be used.\n","        \"\"\"\n","        x = self.data[idx][:-1]\n","        y = self.data[idx][1:]\n","        return (x, y)\n","\n","    def get_block_size(self):\n","        \"\"\"\n","        block_size is the size at which lines are truncated to ensure they are equal-length.\n","        \"\"\"\n","        return self.max_sentence_length\n","    \n","\n","dataset = RecipeDataset(truncation=512) #use this for long\n"],"metadata":{"id":"miOwyY-2m0T9","executionInfo":{"status":"ok","timestamp":1668292445563,"user_tz":300,"elapsed":2097,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c71d9ccb-66ba-41dc-94a0-2786bb965884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /root/.cache/mingpt/encoder.json\n","downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /root/.cache/mingpt/vocab.bpe\n"]}]},{"cell_type":"code","source":["def lm_collate_fn(batch, device):\n","    x = [item[0] for item in batch]  # List (len B) of varying lengths\n","    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n","    maxlen = max([len(s) for s in x])\n","\n","    padded_x, padded_y = [], []\n","    for sx, sy in zip(x, y):\n","        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n","        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n","    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"],"metadata":{"id":"H2o5xwwuq8W8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mingpt.model import GPT\n","\n","model_config = GPT.get_default_config()\n","model_config.model_type = 'gpt-nano'\n","model_config.vocab_size = dataset.get_vocab_size()\n","model_config.block_size = dataset.get_block_size()\n","model_config.n_classification_class = 2\n","model = GPT(model_config)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/ECE1786 Project/model_large100K.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2WjOH4W2A7y","executionInfo":{"status":"ok","timestamp":1668293150576,"user_tz":300,"elapsed":179,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"758a676a-947f-4e32-f397-489253534ea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 2.52M\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Create a Trainer object and set the core hyper-parameters\n","from mingpt.trainer import Trainer\n","\n","train_config = Trainer.get_default_config()\n","train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n","train_config.max_iters = 5000  \n","train_config.num_workers = 0\n","train_config.batch_size = 4    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n","trainer = Trainer(train_config, model, dataset, dataset, collate_fn=lm_collate_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFxW9Pt3sdRO","executionInfo":{"status":"ok","timestamp":1668293152252,"user_tz":300,"elapsed":4,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"14386687-211b-463a-8a24-de855f1af671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["running on device cuda\n"]}]},{"cell_type":"code","source":["# This function is called at the end of every batch in training\n","# and is used to report the amount of time per 100 batches, and the loss at that point\n","\n","def batch_end_callback(trainer):\n","    if trainer.iter_num % 100 == 0:\n","        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","trainer.set_callback('on_batch_end', batch_end_callback)\n","\n","# Train!\n","trainer.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUlfR1e_2HrW","executionInfo":{"status":"ok","timestamp":1668293254432,"user_tz":300,"elapsed":101366,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"aa78a8ad-b838-457a-ee93-dc63f6eda555"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter_dt 0.00ms; iter 0: train loss 7.76950\n","iter_dt 25.92ms; iter 100: train loss 2.10580\n","iter_dt 16.78ms; iter 200: train loss 2.55791\n","iter_dt 20.34ms; iter 300: train loss 1.41126\n","iter_dt 18.74ms; iter 400: train loss 1.06151\n","iter_dt 19.32ms; iter 500: train loss 1.76899\n","iter_dt 17.42ms; iter 600: train loss 1.08196\n","iter_dt 19.99ms; iter 700: train loss 1.29947\n","iter_dt 18.70ms; iter 800: train loss 1.32215\n","iter_dt 16.30ms; iter 900: train loss 1.43644\n","iter_dt 15.81ms; iter 1000: train loss 1.53191\n","iter_dt 18.42ms; iter 1100: train loss 1.20790\n","iter_dt 26.82ms; iter 1200: train loss 1.17187\n","iter_dt 25.62ms; iter 1300: train loss 0.90794\n","iter_dt 25.44ms; iter 1400: train loss 1.44550\n","iter_dt 17.24ms; iter 1500: train loss 0.97430\n","iter_dt 20.18ms; iter 1600: train loss 0.96162\n","iter_dt 33.01ms; iter 1700: train loss 1.16950\n","iter_dt 16.75ms; iter 1800: train loss 0.75089\n","iter_dt 27.26ms; iter 1900: train loss 1.02136\n","iter_dt 35.35ms; iter 2000: train loss 0.84590\n","iter_dt 36.30ms; iter 2100: train loss 1.02921\n","iter_dt 17.15ms; iter 2200: train loss 0.54678\n","iter_dt 15.08ms; iter 2300: train loss 0.97537\n","iter_dt 23.68ms; iter 2400: train loss 1.00896\n","iter_dt 28.88ms; iter 2500: train loss 1.12195\n","iter_dt 27.90ms; iter 2600: train loss 0.97402\n","iter_dt 22.04ms; iter 2700: train loss 1.00347\n","iter_dt 18.69ms; iter 2800: train loss 0.50615\n","iter_dt 14.43ms; iter 2900: train loss 0.73511\n","iter_dt 15.89ms; iter 3000: train loss 0.76831\n","iter_dt 30.21ms; iter 3100: train loss 0.80052\n","iter_dt 16.18ms; iter 3200: train loss 0.76975\n","iter_dt 19.90ms; iter 3300: train loss 1.33678\n","iter_dt 23.79ms; iter 3400: train loss 0.52579\n","iter_dt 19.09ms; iter 3500: train loss 0.45986\n","iter_dt 20.03ms; iter 3600: train loss 0.62011\n","iter_dt 22.53ms; iter 3700: train loss 0.76530\n","iter_dt 18.90ms; iter 3800: train loss 0.56597\n","iter_dt 18.27ms; iter 3900: train loss 0.78439\n","iter_dt 19.16ms; iter 4000: train loss 0.83584\n","iter_dt 38.47ms; iter 4100: train loss 0.86844\n","iter_dt 14.61ms; iter 4200: train loss 0.91165\n","iter_dt 20.48ms; iter 4300: train loss 0.87097\n","iter_dt 19.98ms; iter 4400: train loss 0.57277\n","iter_dt 34.86ms; iter 4500: train loss 0.54720\n","iter_dt 21.24ms; iter 4600: train loss 0.67835\n","iter_dt 19.40ms; iter 4700: train loss 0.58995\n","iter_dt 26.40ms; iter 4800: train loss 0.77942\n","iter_dt 19.85ms; iter 4900: train loss 0.54503\n"]}]},{"cell_type":"code","source":["prompt = \"RECIPE NAME\\nWhiskey Sour \\n\\nRECIPE INGREDIENTS\\n\"\n","encoded_prompt = dataset.tokenizer(prompt).to(trainer.device)\n","generated_sequence = model.generate(encoded_prompt,trainer.device, temperature=0.5, max_new_tokens=100, do_sample=True)\n","print(dataset.tokenizer.decode(generated_sequence[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6cymfPaAK_1","executionInfo":{"status":"ok","timestamp":1668293259693,"user_tz":300,"elapsed":376,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"58639616-1132-46ad-9d3c-57b1e9ab79a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RECIPE NAME\n","Whiskey Sour \n","\n","RECIPE INGREDIENTS\n","2 ounces bourbon \n","1 1/2 ounces Benedictine \n","1/2 ounce lemon juice \n","1/2 ounce lemon juice \n","1/2 ounce syrup \n","lemon twist \n","\n","\n","RECIPE INSTRUCTIONS\n","Add the gin, ginger syrup and honey syrup into a shaker.\n","Add the gin and shake vigorously dry ice and shake until well-chilled.\n","Strain into a chilled coupe glass.\n","*Pour into a lemon twist\n"]}]},{"cell_type":"code","source":["#torch.save(model.state_dict(), \"/content/drive/MyDrive/ECE1786 Project/recipe_baseline.pt\")"],"metadata":{"id":"34jMckOw6m9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mingpt.model import GPT\n","\n","model_config = GPT.get_default_config()\n","model_config.model_type = 'gpt-nano'\n","model_config.vocab_size = dataset.get_vocab_size()\n","model_config.block_size = dataset.get_block_size()\n","model_config.n_classification_class = 2\n","model = GPT(model_config)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/ECE1786 Project/recipe_baseline.pt\"))\n","\n","device = torch.device(\"cpu\")\n","\n","prompt = \"RECIPE NAME\\nWhiskey Sour \\n\\nRECIPE INGREDIENTS\\n\"\n","encoded_prompt = dataset.tokenizer(prompt).to(device)\n","generated_sequence = model.generate(encoded_prompt,device , temperature=0.7, max_new_tokens=100, do_sample=True)\n","print(dataset.tokenizer.decode(generated_sequence[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Ilqjn3w2gmX","executionInfo":{"status":"ok","timestamp":1668293125090,"user_tz":300,"elapsed":1578,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"c0de4450-8819-4772-d0ca-da2cde408768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 2.52M\n","RECIPE NAME\n","Whiskey Sour \n","\n","RECIPE INGREDIENTS\n","1 1/2 ounces amaro \n","1/2 ounce St-proof rum \n","1/2 ounces mezcal \n","lemon twist \n","\n","RECIPE INSTRUCTIONS\n","Add the gin, sweet vermouth, orange liqueur, freshly squeezed into a mixing glass with ice and stir until well-chilled.\n","Strain into a rocks glass over fresh ice.\n","Top with the drink.\n","Garnish with a ground orange twist.\n","*C\n"]}]}]}