{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mrk6IqBwlih9","executionInfo":{"status":"ok","timestamp":1668974923502,"user_tz":300,"elapsed":496,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"51de805a-ebe2-43fc-92d4-3c6d3e786793"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":36}],"source":["# The code below is needed for using Google Colab, so un comment this if that is what you're using\n"," \n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ECE1786 Project/Remy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKB-kEQAEHAz","executionInfo":{"status":"ok","timestamp":1668974927789,"user_tz":300,"elapsed":2918,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"5d74f91b-ff41-4c61-f2ac-4610da65bc42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/ECE1786 Project/Remy\n"]}]},{"cell_type":"code","source":["import torch \n","import numpy as np\n","\n","from nltk.tokenize import sent_tokenize \n","\n","from pathlib import Path \n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from mingpt.bpe import BPETokenizer \n","from mingpt.utils import set_seed \n","import pandas as pd\n","set_seed(1234)"],"metadata":{"id":"wPo9HewGmxcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RecipeDataset(Dataset):\n","    def __init__(self,  truncation=-1):\n","        df = pd.read_pickle(\"/content/drive/MyDrive/ECE1786 Project/Remy/cocktail_dataset.pkl\")  \n","        recipes = []\n","        for i in range(len(df.index)):\n","            recipe = \"RECIPE NAME\\n\" + df.loc[i, \"Name\"] + \" \\n\\nRECIPE INGREDIENTS\\n\"\n","            skip = False\n","            for ingredient in df.loc[i, \"Ingredients\"]:\n","                # remove asterisks\n","                ingredient.replace('*', '')\n","                if \"750\" in ingredient:\n","                    #skip batch drink recipes\n","                    skip = True\n","                recipe += ingredient + \"\\n\"\n","            if skip:\n","                continue\n","            recipe += \"\\nRECIPE INSTRUCTIONS\\n\" \n","            for instruction in  df.loc[i, \"Instructions\"]:\n","                if instruction.startswith(\"*\"):\n","                    continue\n","                recipe += instruction + \"\\n\"\n","            recipes.append(recipe)\n","\n","        # Tokenize\n","        self.tokenizer = BPETokenizer()\n","        self.data = []  # List of 1-d pytorch tensor\n","        for sent in recipes:\n","            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n","            if truncation >= 0:\n","                self.data.append(tokenized[:truncation])\n","            else:\n","                self.data.append(tokenized)\n","\n","        # Count some items\n","        self.max_sentence_length = 512 #np.max([len(d) for d in self.data])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def get_vocab_size(self):\n","        \"\"\"\n","        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), \n","        but actually, only a small number of vocab is used, especially for the small text. \n","        \"\"\"\n","        return 50257\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        The output should be a tuple x and y, both as pytorch tensors.\n","        Please refer to the `run()` method in the mingpt/trainer.py script for \n","        how the x and y are going to be used.\n","        \"\"\"\n","        x = self.data[idx][:-1]\n","        y = self.data[idx][1:]\n","        return (x, y)\n","\n","    def get_block_size(self):\n","        \"\"\"\n","        block_size is the size at which lines are truncated to ensure they are equal-length.\n","        \"\"\"\n","        return self.max_sentence_length\n","    \n","\n","dataset = RecipeDataset(truncation=512)\n","print(len(dataset))\n","\n"],"metadata":{"id":"miOwyY-2m0T9","executionInfo":{"status":"ok","timestamp":1668974989707,"user_tz":300,"elapsed":385,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cfb336d8-fb6d-4d65-8631-3617d16bbce7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["292\n","RECIPE NAME\n","Pineapple Mint Caipirinha \n","\n","RECIPE INGREDIENTS\n","4 1.5-inch pineapple chunks\n","2 mint leaves\n","1 ounce simple syrup\n","2 ounces unaged cachaça\n","Garnish: pineapple wedge\n","\n","RECIPE INSTRUCTIONS\n","In a shaker, muddle the pineapple chunks, mint leaves and simple syrup.\n","Add the cachaça and ice and shake vigorously until well-chilled.\n","Pour (unstrained) into a rocks glass.\n","Garnish with a pineapple wedge.\n"]}]},{"cell_type":"code","source":["print(dataset.tokenizer.decode(dataset[5][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhTWzzoUrG7r","executionInfo":{"status":"ok","timestamp":1668975203276,"user_tz":300,"elapsed":459,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"aef53255-26df-41f0-b50d-04fce8b91623"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RECIPE NAME\n","Red Hook \n","\n","RECIPE INGREDIENTS\n","2 ounces rye whiskey\n","1/2 ounce maraschino liqueur\n","1/2 ounce Punt e Mes\n","Garnish: maraschino cherry\n","\n","RECIPE INSTRUCTIONS\n","Add the rye whiskey, maraschino liqueur and Punt e Mes into a mixing glass with ice and stir until well-chilled.\n","Strain into a cocktail glass.\n","Garnish with a maraschino cherry.\n"]}]},{"cell_type":"code","source":["def lm_collate_fn(batch, device):\n","    x = [item[0] for item in batch]  # List (len B) of varying lengths\n","    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n","    maxlen = max([len(s) for s in x])\n","\n","    padded_x, padded_y = [], []\n","    for sx, sy in zip(x, y):\n","        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n","        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n","    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"],"metadata":{"id":"H2o5xwwuq8W8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mingpt.model import GPT\n","\n","model_config = GPT.get_default_config()\n","model_config.model_type = 'gpt-nano'\n","model_config.vocab_size = dataset.get_vocab_size()\n","model_config.block_size = dataset.get_block_size()\n","model_config.n_classification_class = 2\n","model = GPT(model_config)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/ECE1786 Project/Remy/model_large100K.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2WjOH4W2A7y","executionInfo":{"status":"ok","timestamp":1668974999723,"user_tz":300,"elapsed":773,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"91e271b3-38b9-4b15-d58a-7ba7ce70d626"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 2.52M\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["# Create a Trainer object and set the core hyper-parameters\n","from mingpt.trainer import Trainer\n","\n","train_config = Trainer.get_default_config()\n","train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n","train_config.max_iters = 5000  \n","train_config.num_workers = 0\n","train_config.batch_size = 4    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n","trainer = Trainer(train_config, model, dataset, dataset, collate_fn=lm_collate_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFxW9Pt3sdRO","executionInfo":{"status":"ok","timestamp":1668975002194,"user_tz":300,"elapsed":4,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"a9979ba0-d7a8-4930-919c-3403c2413795"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["running on device cuda\n"]}]},{"cell_type":"code","source":["# This function is called at the end of every batch in training\n","# and is used to report the amount of time per 100 batches, and the loss at that point\n","\n","def batch_end_callback(trainer):\n","    if trainer.iter_num % 100 == 0:\n","        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","trainer.set_callback('on_batch_end', batch_end_callback)\n","\n","# Train!\n","trainer.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUlfR1e_2HrW","executionInfo":{"status":"ok","timestamp":1668975102326,"user_tz":300,"elapsed":90014,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"34449e30-4611-4dde-8357-f41b9d4b0617"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter_dt 0.00ms; iter 0: train loss 8.99142\n","iter_dt 18.49ms; iter 100: train loss 2.85900\n","iter_dt 15.56ms; iter 200: train loss 2.06186\n","iter_dt 18.95ms; iter 300: train loss 2.19352\n","iter_dt 20.52ms; iter 400: train loss 2.16243\n","iter_dt 17.49ms; iter 500: train loss 1.66448\n","iter_dt 14.83ms; iter 600: train loss 1.26252\n","iter_dt 19.04ms; iter 700: train loss 1.68055\n","iter_dt 15.44ms; iter 800: train loss 1.42126\n","iter_dt 17.25ms; iter 900: train loss 0.83758\n","iter_dt 18.99ms; iter 1000: train loss 1.42286\n","iter_dt 17.09ms; iter 1100: train loss 0.75403\n","iter_dt 14.67ms; iter 1200: train loss 1.10615\n","iter_dt 18.78ms; iter 1300: train loss 1.01842\n","iter_dt 20.75ms; iter 1400: train loss 0.81326\n","iter_dt 13.76ms; iter 1500: train loss 0.90966\n","iter_dt 16.17ms; iter 1600: train loss 0.77234\n","iter_dt 18.55ms; iter 1700: train loss 0.86690\n","iter_dt 19.40ms; iter 1800: train loss 0.79430\n","iter_dt 17.61ms; iter 1900: train loss 0.96363\n","iter_dt 14.82ms; iter 2000: train loss 0.96691\n","iter_dt 18.09ms; iter 2100: train loss 0.95022\n","iter_dt 16.77ms; iter 2200: train loss 0.70164\n","iter_dt 14.91ms; iter 2300: train loss 0.84899\n","iter_dt 19.20ms; iter 2400: train loss 1.04519\n","iter_dt 19.01ms; iter 2500: train loss 0.92553\n","iter_dt 14.90ms; iter 2600: train loss 0.82766\n","iter_dt 14.15ms; iter 2700: train loss 1.17626\n","iter_dt 23.28ms; iter 2800: train loss 0.64157\n","iter_dt 16.26ms; iter 2900: train loss 0.66888\n","iter_dt 26.48ms; iter 3000: train loss 0.55981\n","iter_dt 18.54ms; iter 3100: train loss 0.82903\n","iter_dt 15.32ms; iter 3200: train loss 0.71833\n","iter_dt 17.86ms; iter 3300: train loss 0.70685\n","iter_dt 23.53ms; iter 3400: train loss 0.58253\n","iter_dt 23.41ms; iter 3500: train loss 0.44903\n","iter_dt 16.28ms; iter 3600: train loss 0.81261\n","iter_dt 16.44ms; iter 3700: train loss 0.73199\n","iter_dt 15.55ms; iter 3800: train loss 0.47629\n","iter_dt 16.16ms; iter 3900: train loss 0.56302\n","iter_dt 15.91ms; iter 4000: train loss 0.76227\n","iter_dt 20.02ms; iter 4100: train loss 0.36419\n","iter_dt 15.55ms; iter 4200: train loss 0.48547\n","iter_dt 20.03ms; iter 4300: train loss 0.83231\n","iter_dt 15.22ms; iter 4400: train loss 0.68322\n","iter_dt 14.15ms; iter 4500: train loss 0.44511\n","iter_dt 17.65ms; iter 4600: train loss 0.46789\n","iter_dt 18.61ms; iter 4700: train loss 0.59071\n","iter_dt 16.53ms; iter 4800: train loss 0.45673\n","iter_dt 15.99ms; iter 4900: train loss 0.52691\n"]}]},{"cell_type":"code","source":["prompt = \"RECIPE NAME\\nWhiskey Sour \\n\\nRECIPE INGREDIENTS\\n\"\n","encoded_prompt = dataset.tokenizer(prompt).to(trainer.device)\n","generated_sequence = model.generate(encoded_prompt,trainer.device, temperature=0.5, max_new_tokens=100, do_sample=True)\n","print(dataset.tokenizer.decode(generated_sequence[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6cymfPaAK_1","executionInfo":{"status":"ok","timestamp":1668975174073,"user_tz":300,"elapsed":665,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"6dc1e5e8-412c-42f4-980f-97c0f14d2891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RECIPE NAME\n","Whiskey Sour \n","\n","RECIPE INGREDIENTS\n","1 ounce whiskey\n","1/2 ounce lemon juice, freshly squeezed\n","2 ounce simple syrup\n","2 ounce or to water\n","Garnish: lemon twist\n","\n","\n","RECIPE INSTRUCTIONS\n","Add the bourbon, lemon and lemon juice to a shaker with ice, and shake until well-chilled.\n","Strain into a chilled rocks glass.\n","Garnish with a lemon twist.\n","Garnish with a lime twist.\n","Garnish with a lemon twist.\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/ECE1786 Project/Remy/recipe_baseline.pt\")"],"metadata":{"id":"34jMckOw6m9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mingpt.model import GPT\n","\n","model_config = GPT.get_default_config()\n","model_config.model_type = 'gpt-nano'\n","model_config.vocab_size = dataset.get_vocab_size()\n","model_config.block_size = dataset.get_block_size()\n","model_config.n_classification_class = 2\n","model = GPT(model_config)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/ECE1786 Project/Remy/recipe_baseline.pt\"))\n","\n","device = torch.device(\"cpu\")\n","\n","prompt = \"RECIPE NAME\\nWhiskey Sour \\n\\nRECIPE INGREDIENTS\\n\"\n","encoded_prompt = dataset.tokenizer(prompt).to(device)\n","generated_sequence = model.generate(encoded_prompt,device , temperature=0.7, max_new_tokens=100, do_sample=True)\n","print(dataset.tokenizer.decode(generated_sequence[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Ilqjn3w2gmX","executionInfo":{"status":"ok","timestamp":1668293125090,"user_tz":300,"elapsed":1578,"user":{"displayName":"Wendy Wang","userId":"05544186475574668036"}},"outputId":"c0de4450-8819-4772-d0ca-da2cde408768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 2.52M\n","RECIPE NAME\n","Whiskey Sour \n","\n","RECIPE INGREDIENTS\n","1 1/2 ounces amaro \n","1/2 ounce St-proof rum \n","1/2 ounces mezcal \n","lemon twist \n","\n","RECIPE INSTRUCTIONS\n","Add the gin, sweet vermouth, orange liqueur, freshly squeezed into a mixing glass with ice and stir until well-chilled.\n","Strain into a rocks glass over fresh ice.\n","Top with the drink.\n","Garnish with a ground orange twist.\n","*C\n"]}]}]}