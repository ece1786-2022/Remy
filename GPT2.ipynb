{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci6SNKaOzhC1",
        "outputId": "b07fff91-d468-469c-9e5e-990613bac7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/ece1786-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5qBxcNA0jC0",
        "outputId": "ed10006d-e087-44e7-9506-2f156950c85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/ece1786-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "58P8xnILzla1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U-j5tjozbrT"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"gpt2\" #\"distilgpt2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)"
      ],
      "metadata": {
        "id": "Rk9ae5hFzzkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_pickle(\"cocktail_dataset.pkl\")  \n",
        "recipes = []\n",
        "for i in range(len(df.index)):\n",
        "  recipe = \"RECIPE NAME\\n\" + df.loc[i, \"Name\"] + \" \\n\\nRECIPE INGREDIENTS\\n\"\n",
        "  skip = False\n",
        "  for ingredient in df.loc[i, \"Ingredients\"]:\n",
        "    # remove asterisks\n",
        "    ingredient.replace('*', '')\n",
        "    if \"750\" in ingredient:\n",
        "      #skip batch drink recipes\n",
        "      skip = True\n",
        "    recipe += ingredient + \"\\n\"\n",
        "  if skip:\n",
        "    continue\n",
        "  recipe += \"\\nRECIPE INSTRUCTIONS\\n\" \n",
        "  for instruction in  df.loc[i, \"Instructions\"]:\n",
        "    if instruction.startswith(\"*\"):\n",
        "      continue\n",
        "    recipe += instruction + \"\\n\"\n",
        "  recipes.append(recipe)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data = tokenizer(recipes, padding='longest')\n",
        "data[\"labels\"] = data[\"input_ids\"].copy()\n",
        "print(data.keys())\n",
        "dataset = Dataset.from_dict(data)\n",
        "print(len(dataset))\n",
        "# print(dataset[5])\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1blwHCDd0SfZ",
        "outputId": "9ca32325-4b4b-4f79-b01e-2b774e6a5f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "292\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 292\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset[5]['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuZre6YzO9Sy",
        "outputId": "e9799aee-7bf7-4088-bfd8-d8433f6407ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Model\n",
        "from transformers import Trainer, TrainingArguments\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-recipes\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=50,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPRSLZlh4ZRj",
        "outputId": "96025871-c248-4b93-a4fa-f1f1d33d1e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/0dd7bcc7a64e4350d8859c9a2813132fbf6ae591/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/0dd7bcc7a64e4350d8859c9a2813132fbf6ae591/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "HAM0QLeW4iQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T0jFtHDE4l6C",
        "outputId": "a640a90e-054b-4810-ea06-94fadd6fb157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 292\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1850\n",
            "  Number of trainable parameters = 124439808\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1850' max='1850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1850/1850 29:48, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.842600</td>\n",
              "      <td>0.816406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.561100</td>\n",
              "      <td>0.683949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.634800</td>\n",
              "      <td>0.624084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.526300</td>\n",
              "      <td>0.586055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.694700</td>\n",
              "      <td>0.558562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.975500</td>\n",
              "      <td>0.534396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.514674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.524300</td>\n",
              "      <td>0.495443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.479761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.464749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.499000</td>\n",
              "      <td>0.449660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.354400</td>\n",
              "      <td>0.435943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.595000</td>\n",
              "      <td>0.423632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.453500</td>\n",
              "      <td>0.412736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.401192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.426500</td>\n",
              "      <td>0.391667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.450300</td>\n",
              "      <td>0.382310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.613000</td>\n",
              "      <td>0.372935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.326200</td>\n",
              "      <td>0.364104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.722700</td>\n",
              "      <td>0.355414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.331900</td>\n",
              "      <td>0.345549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.461100</td>\n",
              "      <td>0.338804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.425900</td>\n",
              "      <td>0.332003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.322000</td>\n",
              "      <td>0.325733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.220800</td>\n",
              "      <td>0.318613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.353900</td>\n",
              "      <td>0.312581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.453900</td>\n",
              "      <td>0.305581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.504900</td>\n",
              "      <td>0.299946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.269700</td>\n",
              "      <td>0.295839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.291521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.372200</td>\n",
              "      <td>0.286512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.390600</td>\n",
              "      <td>0.283461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.248100</td>\n",
              "      <td>0.278192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.508400</td>\n",
              "      <td>0.273851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.477700</td>\n",
              "      <td>0.271223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.451200</td>\n",
              "      <td>0.267699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.355700</td>\n",
              "      <td>0.264521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.244300</td>\n",
              "      <td>0.261468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.335900</td>\n",
              "      <td>0.258242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.413700</td>\n",
              "      <td>0.256158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.398900</td>\n",
              "      <td>0.254706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.409200</td>\n",
              "      <td>0.253142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.327000</td>\n",
              "      <td>0.251718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.205600</td>\n",
              "      <td>0.250205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.286700</td>\n",
              "      <td>0.248877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.301700</td>\n",
              "      <td>0.247670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.267500</td>\n",
              "      <td>0.246861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.365000</td>\n",
              "      <td>0.246558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.370700</td>\n",
              "      <td>0.246156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.245962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to gpt2-finetuned-recipes/checkpoint-500\n",
            "Configuration saved in gpt2-finetuned-recipes/checkpoint-500/config.json\n",
            "Model weights saved in gpt2-finetuned-recipes/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to gpt2-finetuned-recipes/checkpoint-1000\n",
            "Configuration saved in gpt2-finetuned-recipes/checkpoint-1000/config.json\n",
            "Model weights saved in gpt2-finetuned-recipes/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to gpt2-finetuned-recipes/checkpoint-1500\n",
            "Configuration saved in gpt2-finetuned-recipes/checkpoint-1500/config.json\n",
            "Model weights saved in gpt2-finetuned-recipes/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 292\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1850, training_loss=0.45332525818734554, metrics={'train_runtime': 1788.9955, 'train_samples_per_second': 8.161, 'train_steps_per_second': 1.034, 'total_flos': 2376838886400000.0, 'train_loss': 0.45332525818734554, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"RECIPE NAME\\nMagnum Opus\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "outputs = model.generate(input_ids, do_sample=True, max_length=100,temperature = 0.8, top_p = 1, return_dict_in_generate=True, output_scores=True)\n",
        "print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5by2-oGN87MR",
        "outputId": "d02b0712-866d-4860-f022-11ff68451c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECIPE NAME\n",
            "Magnum Opus Colada \n",
            "\n",
            "RECIPE INGREDIENTS\n",
            "2 ounces Aurora CBD-infused rum*\n",
            "1 ounce St. George Spirits vodka\n",
            "1/4 ounce St-Germain elderflower liqueur (equal parts St-Germain and St-Germain elderflower oils)\n",
            "Garnish: lemon twist\n",
            "\n",
            "RECIPE INSTRUCTIONS\n",
            "Add all ingredients into a shaker with ice and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gpt2model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMf7Ubl8d42m",
        "outputId": "467fa220-2c9b-4c99-cbc8-dcae7fa5ef54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to gpt2model\n",
            "Configuration saved in gpt2model/config.json\n",
            "Model weights saved in gpt2model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = AutoModelForCausalLM.from_pretrained(\"./gpt2model\").to(\"cuda:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F72KMz0Fj1O9",
        "outputId": "210ab1cc-2a61-4145-9a62-955c97ba8aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./gpt2model/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"./gpt2model\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file ./gpt2model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./gpt2model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"RECIPE NAME\\nMagnum Opus\"\n",
        "input_ids1 = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "outputs1 = model1.generate(input_ids1, do_sample=True, max_length=100, temperature = 0.8, top_p = 1, return_dict_in_generate=True, output_scores=True)\n",
        "print(tokenizer.batch_decode(outputs1.sequences, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyHuxJgNkDH7",
        "outputId": "f25d841d-3408-48b1-f99b-1c48055a6ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECIPE NAME\n",
            "Magnum Opus Domaine \n",
            "\n",
            "RECIPE INGREDIENTS\n",
            "1 small creme de menthe\n",
            "1 ounce creme de cacao\n",
            "2 ounces white rum\n",
            "1 ounce peach schnapps\n",
            "1 ounce lemon juice, freshly squeezed\n",
            "Garnish: brandied cherry\n",
            "Garnish: orange slices\n",
            "\n",
            "RECIPE INSTRUCTIONS\n",
            "Add the creme de menthe, cacao, white rum, peach schnapps\n"
          ]
        }
      ]
    }
  ]
}